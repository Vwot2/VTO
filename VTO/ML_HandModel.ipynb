{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887a1a4d-edc2-4111-9925-5eceb7277bb4",
   "metadata": {},
   "source": [
    "## Hand Pose Detection Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2dfed-de52-4cef-bb2a-f2964b373776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class HandDatasetPreprocessor:\n",
    "    def __init__(self, config_path='config.yaml', hand_img_dir=None, annotations_dir=None, \n",
    "                 non_hand_dir=None, max_samples=6000, max_negative_samples=500):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        self.hand_img_dir = hand_img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.non_hand_dir = non_hand_dir\n",
    "        self.output_dir = os.path.abspath(self.config['paths']['output_dir'])\n",
    "        self.dataset_dir = os.path.join(self.output_dir, 'hand_dataset')\n",
    "        self.max_samples = max_samples\n",
    "        self.max_negative_samples = max_negative_samples\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            os.makedirs(os.path.join(self.dataset_dir, split, 'images'), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.dataset_dir, split, 'labels'), exist_ok=True)\n",
    "\n",
    "    def _get_negative_samples(self):\n",
    "        \"\"\"Gets list of negative sample images.\"\"\"\n",
    "        if not self.non_hand_dir:\n",
    "            return []\n",
    "        \n",
    "        negative_samples = []\n",
    "        valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "        \n",
    "        for file in os.listdir(self.non_hand_dir):\n",
    "            if file.lower().endswith(valid_extensions):\n",
    "                img_path = os.path.join(self.non_hand_dir, file)\n",
    "                try:\n",
    "                    # Verify the image can be opened\n",
    "                    image = cv2.imread(img_path)\n",
    "                    if image is not None:\n",
    "                        negative_samples.append({\n",
    "                            'image_path': img_path,\n",
    "                            'is_negative': True\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading negative sample {file}: {e}\")\n",
    "        \n",
    "        print(f\"Found {len(negative_samples)} negative samples.\")\n",
    "        return negative_samples[:self.max_negative_samples]\n",
    "\n",
    "    def _get_image_annotation_pairs(self):\n",
    "        \"\"\"Pairs each image with its annotation file.\"\"\"\n",
    "        if not self.hand_img_dir or not self.annotations_dir:\n",
    "            raise ValueError(\"Image and annotations directories must be specified.\")\n",
    "        \n",
    "        pairs = []\n",
    "        for file in os.listdir(self.hand_img_dir):\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                img_path = os.path.join(self.hand_img_dir, file)\n",
    "                annotation_name = os.path.splitext(file)[0] + '.txt'\n",
    "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "                \n",
    "                if os.path.exists(annotation_path):\n",
    "                    pairs.append((img_path, annotation_path))\n",
    "                else:\n",
    "                    print(f\"Warning: No annotation found for {file}\")\n",
    "        \n",
    "        print(f\"Found {len(pairs)} positive image-annotation pairs.\")\n",
    "        return pairs[:self.max_samples]\n",
    "\n",
    "    def _validate_keypoints(self, line):\n",
    "        \"\"\"Validates the simplified keypoint format (8 values).\"\"\"\n",
    "        try:\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # Check if we have exactly 8 values\n",
    "            if len(parts) != 8:\n",
    "                print(f\"Invalid number of values in line. Expected 8, got {len(parts)}\")\n",
    "                return None\n",
    "            \n",
    "            # Parse values\n",
    "            class_id = int(parts[0])\n",
    "            bbox = [float(x) for x in parts[1:5]]\n",
    "            keypoint = [float(x) for x in parts[5:8]]\n",
    "            \n",
    "            # Validate class ID\n",
    "            if class_id != 0:\n",
    "                print(f\"Invalid class ID: {class_id}\")\n",
    "                return None\n",
    "                \n",
    "            # Validate bbox values are between 0 and 1\n",
    "            if not all(0 <= x <= 1 for x in bbox):\n",
    "                print(f\"Invalid bbox values (must be between 0 and 1): {bbox}\")\n",
    "                return None\n",
    "            \n",
    "            # Validate keypoint x,y values are between 0 and 1\n",
    "            if not all(0 <= x <= 1 for x in keypoint[:2]):\n",
    "                print(f\"Invalid keypoint coordinates (must be between 0 and 1): {keypoint[:2]}\")\n",
    "                return None\n",
    "            \n",
    "            return line.strip()\n",
    "            \n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Error processing keypoint line: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        \"\"\"Prepares samples with validated keypoint annotations and negative samples.\"\"\"\n",
    "        print(\"Preparing samples...\")\n",
    "        pairs = self._get_image_annotation_pairs()\n",
    "        negative_samples = self._get_negative_samples()\n",
    "        samples = []\n",
    "        invalid_count = 0\n",
    "        \n",
    "        # Process positive samples\n",
    "        for img_path, ann_path in pairs:\n",
    "            try:\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Failed to read image: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                with open(ann_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                valid_annotations = []\n",
    "                for line in lines:\n",
    "                    formatted_line = self._validate_keypoints(line)\n",
    "                    if formatted_line:\n",
    "                        valid_annotations.append(formatted_line)\n",
    "                    else:\n",
    "                        invalid_count += 1\n",
    "                \n",
    "                if valid_annotations:\n",
    "                    samples.append({\n",
    "                        'image_path': img_path,\n",
    "                        'yolo_format': valid_annotations,\n",
    "                        'is_negative': False\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"No valid annotations found in {ann_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Add negative samples\n",
    "        samples.extend(negative_samples)\n",
    "        \n",
    "        print(f\"Processed total {len(samples)} samples:\")\n",
    "        print(f\"- Valid positive samples: {len(samples) - len(negative_samples)}\")\n",
    "        print(f\"- Negative samples: {len(negative_samples)}\")\n",
    "        print(f\"- Invalid annotations: {invalid_count}\")\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"Splits the dataset into train, val, and test sets.\"\"\"\n",
    "        all_samples = self._prepare_samples()\n",
    "        \n",
    "        if not all_samples:\n",
    "            raise ValueError(\"No valid samples found in the dataset.\")\n",
    "        \n",
    "        train_ratio = self.config['training']['train_ratio']\n",
    "        val_ratio = self.config['training']['val_ratio']\n",
    "        \n",
    "        train_data, temp = train_test_split(all_samples, train_size=train_ratio, \n",
    "                                          stratify=[s['is_negative'] for s in all_samples])\n",
    "        val_data, test_data = train_test_split(temp, train_size=val_ratio/(1-train_ratio),\n",
    "                                             stratify=[s['is_negative'] for s in temp])\n",
    "        \n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def prepare_yolo_dataset(self):\n",
    "        \"\"\"Prepares the dataset in YOLO format.\"\"\"\n",
    "        train_data, val_data, test_data = self.prepare_dataset()\n",
    "        splits = {'train': train_data, 'val': val_data, 'test': test_data}\n",
    "        \n",
    "        total_processed = 0\n",
    "        total_negative = 0\n",
    "        \n",
    "        for split_name, split_data in splits.items():\n",
    "            print(f\"\\nProcessing {split_name} split: {len(split_data)} samples\")\n",
    "            \n",
    "            for idx, sample in enumerate(split_data):\n",
    "                try:\n",
    "                    # Copy and rename image\n",
    "                    src_path = sample['image_path']\n",
    "                    dst_name = f\"{idx:06d}.jpg\"\n",
    "                    dst_path = os.path.join(self.dataset_dir, split_name, 'images', dst_name)\n",
    "                    \n",
    "                    image = cv2.imread(src_path)\n",
    "                    if image is None:\n",
    "                        print(f\"Failed to read image: {src_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    cv2.imwrite(dst_path, image)\n",
    "                    \n",
    "                    # Save annotations (empty file for negative samples)\n",
    "                    label_path = os.path.join(self.dataset_dir, split_name, 'labels', f\"{idx:06d}.txt\")\n",
    "                    if sample['is_negative']:\n",
    "                        # Create empty label file for negative sample\n",
    "                        open(label_path, 'w').close()\n",
    "                        total_negative += 1\n",
    "                    else:\n",
    "                        # Write annotations for positive sample\n",
    "                        with open(label_path, 'w') as f:\n",
    "                            for line in sample['yolo_format']:\n",
    "                                f.write(line + '\\n')\n",
    "                    \n",
    "                    total_processed += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {src_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(f\"\\nTotal processed samples: {total_processed}\")\n",
    "        print(f\"- Positive samples: {total_processed - total_negative}\")\n",
    "        print(f\"- Negative samples: {total_negative}\")\n",
    "        \n",
    "        # Create data.yaml\n",
    "        data_yaml = {\n",
    "            'path': self.dataset_dir,\n",
    "            'train': os.path.join('train', 'images'),\n",
    "            'val': os.path.join('val', 'images'),\n",
    "            'test': os.path.join('test', 'images'),\n",
    "            'nc': 1,\n",
    "            'names': ['hand'],\n",
    "            'kpt_shape': [1, 3],  # [number of keypoints, dimensions per keypoint]\n",
    "            'task': 'pose'\n",
    "        }\n",
    "        \n",
    "        yaml_path = os.path.join(self.dataset_dir, 'data.yaml')\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(data_yaml, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"\\nDataset configuration saved to: {yaml_path}\")\n",
    "        return yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea76446-ee33-40d5-be6b-70fc092c0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = HandDatasetPreprocessor(\n",
    "    config_path='config.yaml',\n",
    "    hand_img_dir= r\"C:/Users/dogsh/Documents/Work/MachineLearning/Hand_wrist_keypoint/train/images\",\n",
    "    annotations_dir= r\"C:/Users/dogsh/Documents/Work/MachineLearning/Hand_wrist_keypoint/train/labels\",\n",
    "    non_hand_dir= r\"C:\\Users\\dogsh\\Documents\\Work\\MachineLearning\\Hand_wrist_keypoint\\non-hands\",\n",
    "    max_samples=6000,\n",
    "    max_negative_samples=500\n",
    ")\n",
    "preprocessor.prepare_yolo_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f249c33-4e73-47d9-b032-1764d4ed6379",
   "metadata": {},
   "source": [
    "## Machine Learning Hand Keypoint Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880fb55-d185-4c77-a00d-2654526b318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class HandWristDetector:\n",
    "    def __init__(self, config_path='config.yaml'):\n",
    "        \"\"\"\n",
    "        Initialize HandWristDetector with configuration\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): Path to the configuration YAML file\n",
    "        \"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        # Initialize YOLO pose detection model\n",
    "        model_size = self.config['model']['size']\n",
    "        model_path = f\"yolov8{model_size}-pose.pt\"\n",
    "        \n",
    "        # Download model if not exists\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Downloading YOLOv8{model_size} pose model...\")\n",
    "        \n",
    "        self.model = YOLO(model_path)\n",
    "        \n",
    "    def train(self, data_yaml):\n",
    "        \"\"\"\n",
    "        Train the model with custom configuration\n",
    "        \n",
    "        Args:\n",
    "            data_yaml (str): Path to the data YAML file containing dataset configuration\n",
    "            \n",
    "        Returns:\n",
    "            results: Training results object\n",
    "        \"\"\"\n",
    "        # Set training arguments\n",
    "        args = dict(\n",
    "            data=data_yaml,                    # Path to data YAML file\n",
    "            task='pose',                       # Task type for pose detection\n",
    "            mode='train',                      # Training mode\n",
    "            model=self.model,                  # Model to train\n",
    "            epochs=self.config['model']['epochs'],\n",
    "            imgsz=self.config['model']['image_size'],\n",
    "            batch=self.config['model']['batch_size'],\n",
    "            device='',                         # Device to use (auto-select)\n",
    "            workers=8,                         # Number of worker threads\n",
    "            optimizer='AdamW',                  # Optimizer to use (Adam)\n",
    "            patience=20,                       # Early stopping patience\n",
    "            verbose=True,                      # Print verbose output\n",
    "            seed=0,                           # Random seed\n",
    "            deterministic=True,                # Enable deterministic mode\n",
    "            single_cls=True,                   # Single class training\n",
    "            rect=True,                         # Rectangular training\n",
    "            cos_lr=True,                       # Cosine learning rate scheduler\n",
    "            close_mosaic=10,                   # Disable mosaic augmentation for final epochs\n",
    "            resume=False,                      # Resume training\n",
    "            amp=True,                          # Automatic Mixed Precision\n",
    "            \n",
    "            # Learning rate settings\n",
    "            lr0=0.001,                        # Initial learning rate\n",
    "            lrf=0.01,                         # Final learning rate fraction\n",
    "            momentum=0.937,                    # SGD momentum/Adam beta1\n",
    "            weight_decay=0.0005,              # Optimizer weight decay\n",
    "            warmup_epochs=3.0,                # Warmup epochs\n",
    "            warmup_momentum=0.8,              # Warmup initial momentum\n",
    "            warmup_bias_lr=0.1,               # Warmup initial bias learning rate\n",
    "            \n",
    "            # Loss coefficients\n",
    "            box=7.5,                          # Box loss gain\n",
    "            cls=0.5,                          # Class loss gain\n",
    "            pose=12.0,                        # Pose loss gain\n",
    "            kobj=2.0,                         # Keypoint obj loss gain\n",
    "            \n",
    "            # Augmentation settings\n",
    "            degrees=10.0,                      # Rotation degrees\n",
    "            translate=0.2,                    # Translation\n",
    "            scale=0.7,                        # Scale\n",
    "            fliplr=0.5,                       # Horizontal flip probability\n",
    "            mosaic=1.0,                       # Mosaic probability\n",
    "            mixup=0.0,                        # Mixup probability\n",
    "            \n",
    "            # Saving settings\n",
    "            project='runs/pose',              # Project name\n",
    "            name='train',                     # Run name\n",
    "            exist_ok=False,                   # Allow existing project\n",
    "            pretrained=True,                  # Use pretrained model\n",
    "            plots=True,                       # Generate plots\n",
    "            save=True,                        # Save train checkpoints\n",
    "            save_period=-1,                   # Save checkpoint every x epochs\n",
    "            \n",
    "            # Validation settings\n",
    "            val=True,                         # Validate during training\n",
    "            save_json=False,                  # Save JSON validation results\n",
    "            conf=None,                        # Confidence threshold\n",
    "            iou=0.7,                          # NMS IoU threshold\n",
    "            max_det=300,                      # Maximum detections per image\n",
    "            \n",
    "            # Advanced settings\n",
    "            fraction=1.0,                     # Dataset fraction to train on\n",
    "            profile=False,                    # Profile ONNX/TF.js/TensorRT\n",
    "            overlap_mask=True,                # Masks should overlap during inference\n",
    "            mask_ratio=4,                     # Mask downsample ratio\n",
    "            dropout=0.2,                      # Use dropout regularization\n",
    "            label_smoothing=0.1,              # Label smoothing epsilon\n",
    "            nbs=64,                          # Nominal batch size\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        try:\n",
    "            results = self.model.train(**args)\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate(self, data_yaml):\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation/test set\n",
    "        \n",
    "        Args:\n",
    "            data_yaml (str): Path to the data YAML file\n",
    "            \n",
    "        Returns:\n",
    "            results: Validation results object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.model.val(\n",
    "                data=data_yaml,\n",
    "                imgsz=self.config['model']['image_size'],\n",
    "                batch=self.config['model']['batch_size'],\n",
    "                conf=0.25,\n",
    "                iou=0.7,\n",
    "                device='',\n",
    "                verbose=True,\n",
    "                save_json=False,\n",
    "                save_hybrid=False,\n",
    "                max_det=300,\n",
    "                half=False\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def export_model(self, format='onnx'):\n",
    "        \"\"\"\n",
    "        Export the model to specified format\n",
    "        \n",
    "        Args:\n",
    "            format (str): Format to export to ('onnx' or 'tflite')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if format == 'onnx':\n",
    "                self.model.export(\n",
    "                    format='onnx',\n",
    "                    dynamic=True,\n",
    "                    simplify=True,\n",
    "                    opset=11,\n",
    "                    device='cpu'\n",
    "                )\n",
    "            elif format == 'tflite':\n",
    "                self.model.export(\n",
    "                    format='tflite',\n",
    "                    int8=True,\n",
    "                    device='cpu'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Export error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"\n",
    "        Run inference on a single image\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the input image\n",
    "            \n",
    "        Returns:\n",
    "            results: Detection results object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.model.predict(\n",
    "                source=image_path,\n",
    "                conf=0.25,\n",
    "                iou=0.45,\n",
    "                imgsz=self.config['model']['image_size'],\n",
    "                device='',\n",
    "                verbose=False,\n",
    "                save=True,\n",
    "                save_txt=False,\n",
    "                save_conf=False,\n",
    "                save_crop=False,\n",
    "                show_labels=True,\n",
    "                show_conf=True,\n",
    "                max_det=300,\n",
    "                agnostic_nms=False,\n",
    "                classes=None,\n",
    "                retina_masks=False,\n",
    "                boxes=True\n",
    "            )\n",
    "            return results[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def predict_batch(self, image_paths):\n",
    "        \"\"\"\n",
    "        Run inference on a batch of images\n",
    "        \n",
    "        Args:\n",
    "            image_paths (list): List of paths to input images\n",
    "            \n",
    "        Returns:\n",
    "            results: List of detection results objects\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.model.predict(\n",
    "                source=image_paths,\n",
    "                conf=0.25,\n",
    "                iou=0.45,\n",
    "                imgsz=self.config['model']['image_size'],\n",
    "                batch=self.config['model']['batch_size']\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Batch prediction error: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14f954-cef4-4d1f-929d-3778155cd4d5",
   "metadata": {},
   "source": [
    "## Machine Learning Hand Detection Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ad87c-2d73-43e3-8f6f-c7033449097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use current working directory\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "DEFAULT_CONFIG = os.path.join(SCRIPT_DIR, 'config.yaml')\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging configuration\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(f'logs/hand_detection_{timestamp}.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load and validate configuration\"\"\"\n",
    "    try:\n",
    "        with open(DEFAULT_CONFIG, 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Validate paths\n",
    "        required_paths = ['output_dir', 'hand_img_dir', 'non_hand_dir', 'annotations_dir']\n",
    "        for path_key in required_paths:\n",
    "            path_value = config['paths'].get(path_key)\n",
    "            if not path_value or not os.path.exists(path_value):\n",
    "                raise FileNotFoundError(f\"Path '{path_key}' is invalid or does not exist: {path_value}\")\n",
    "        \n",
    "        return config\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading config: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\"Prepare the dataset in YOLO format\"\"\"\n",
    "    logger.info(\"Starting dataset preparation...\")\n",
    "    config = load_config()\n",
    "    preprocessor = HandDatasetPreprocessor(\n",
    "        config_path=DEFAULT_CONFIG,\n",
    "        hand_img_dir=config['paths']['hand_img_dir'],\n",
    "        non_hand_dir=config['paths']['non_hand_dir'],\n",
    "        annotations_dir=config['paths']['annotations_dir'],\n",
    "        max_samples=7000,\n",
    "        max_negative_samples=1000\n",
    "    )\n",
    "    \n",
    "    data_yaml_path = preprocessor.prepare_yolo_dataset()\n",
    "    logger.info(f\"Dataset prepared in YOLO format at {data_yaml_path}\")\n",
    "    return data_yaml_path\n",
    "\n",
    "def train_model(data_yaml):\n",
    "    \"\"\"Train the hand detection model\"\"\"\n",
    "    logger.info(\"Starting model training...\")\n",
    "\n",
    "    if not os.path.exists(data_yaml):\n",
    "        raise FileNotFoundError(f\"Data YAML not found at {data_yaml}\")\n",
    "\n",
    "    detector = HandWristDetector(config_path=DEFAULT_CONFIG)\n",
    "\n",
    "    results = detector.train(data_yaml)\n",
    "    \n",
    "    logger.info(\"Training is not implemented in this example.\")\n",
    "    return None, None\n",
    "\n",
    "def run_pipeline(mode='train', image_path=None):\n",
    "    \"\"\"\n",
    "    Run the hand detection pipeline in different modes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config = load_config()\n",
    "\n",
    "        if mode == 'debug':\n",
    "            logger.info(\"Debugging dataset preparation...\")\n",
    "            preprocessor = HandDatasetPreprocessor(\n",
    "                config_path=DEFAULT_CONFIG,\n",
    "                hand_img_dir=config['paths']['hand_img_dir'],\n",
    "                non_hand_dir=config['paths']['non_hand_dir'],\n",
    "                annotations_dir=config['paths']['annotations_dir'],\n",
    "                max_samples=7000,\n",
    "                max_negative_samples=1000\n",
    "            )\n",
    "            preprocessor._prepare_samples()\n",
    "        \n",
    "        elif mode == 'train':\n",
    "            logger.info(\"Starting training pipeline...\")\n",
    "            data_yaml = prepare_dataset()\n",
    "            detector, results = train_model(data_yaml)\n",
    "            logger.info(\"Training completed.\")\n",
    "        \n",
    "        elif mode == 'inference':\n",
    "            if not image_path:\n",
    "                raise ValueError(\"Image path must be provided for inference mode\")\n",
    "            logger.info(f\"Inference on image: {image_path}\")\n",
    "            logger.info(\"Inference is not implemented in this example.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline(mode='train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d6fa8-26e4-4f15-9f5d-abb364a1496f",
   "metadata": {},
   "source": [
    "## Webcam script to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe213e0-6e22-4dbe-ba85-17f133f652d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class WebcamDetector:\n",
    "    def __init__(self, config_path='config.yaml'):\n",
    "        \"\"\"Initialize the webcam detector with configuration\"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.cap = None\n",
    "        self.frame_count = 0\n",
    "        self.fps = 0\n",
    "        self.last_time = time.time()\n",
    "        \n",
    "        # Initialize the hand detector model with custom weights\n",
    "        try:\n",
    "            # Use the last trained weights from your output directory\n",
    "            weights_path = os.path.join(\n",
    "                self.config['paths']['output_dir'],\n",
    "                'runs/pose/train11/weights/best.pt'\n",
    "            )\n",
    "            if not os.path.exists(weights_path):\n",
    "                raise FileNotFoundError(f\"Weights file not found at {weights_path}\")\n",
    "            \n",
    "            self.model = YOLO(weights_path)\n",
    "            self.logger.info(\"Custom hand detection model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _load_config(self, config_path):\n",
    "        \"\"\"Load configuration file\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading config: {e}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_camera(self, camera_id=0):\n",
    "        \"\"\"Initialize the webcam\"\"\"\n",
    "        self.cap = cv2.VideoCapture(camera_id)\n",
    "        if not self.cap.isOpened():\n",
    "            raise ValueError(f\"Could not open camera {camera_id}\")\n",
    "        \n",
    "        # Set camera properties\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "        self.logger.info(\"Camera initialized successfully\")\n",
    "\n",
    "    def calculate_fps(self):\n",
    "        \"\"\"Calculate FPS\"\"\"\n",
    "        self.frame_count += 1\n",
    "        if (time.time() - self.last_time) > 1.0:\n",
    "            self.fps = self.frame_count\n",
    "            self.frame_count = 0\n",
    "            self.last_time = time.time()\n",
    "        return self.fps\n",
    "\n",
    "    def draw_detections(self, frame, result):\n",
    "        \"\"\"Draw hand detections and wrist keypoints\"\"\"\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        if result.keypoints is not None:\n",
    "            keypoints = result.keypoints.data\n",
    "            boxes = result.boxes.data\n",
    "            \n",
    "            for box, kpts in zip(boxes, keypoints):\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box[:4])\n",
    "                conf = float(box[4])\n",
    "                \n",
    "                # Draw bounding box for hand\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add confidence score\n",
    "                cv2.putText(annotated_frame, f\"Hand: {conf:.2f}\", \n",
    "                           (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                           0.5, (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw wrist keypoint\n",
    "                for kpt in kpts:\n",
    "                    x, y = map(int, kpt[:2])\n",
    "                    conf = float(kpt[2])\n",
    "                    if conf > 0.5:  # Only draw high-confidence keypoints\n",
    "                        cv2.circle(annotated_frame, (x, y), 5, (255, 0, 0), -1)\n",
    "                        cv2.putText(annotated_frame, f\"Wrist: {conf:.2f}\", \n",
    "                                  (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                  0.5, (255, 0, 0), 2)\n",
    "        \n",
    "        # Add FPS counter\n",
    "        fps = self.calculate_fps()\n",
    "        cv2.putText(annotated_frame, f'FPS: {fps}', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        return annotated_frame\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process each frame for hand detection\"\"\"\n",
    "        try:\n",
    "            # Run inference\n",
    "            results = self.model.predict(\n",
    "                source=frame,\n",
    "                conf=0.25,  # Confidence threshold\n",
    "                iou=0.45,   # NMS IoU threshold\n",
    "                verbose=False,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            # Get the first result\n",
    "            result = next(results)\n",
    "            \n",
    "            # Draw detections\n",
    "            annotated_frame = self.draw_detections(frame, result)\n",
    "            \n",
    "            return annotated_frame\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing frame: {e}\")\n",
    "            return frame\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop for webcam detection\"\"\"\n",
    "        try:\n",
    "            self.initialize_camera()\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    self.logger.error(\"Failed to grab frame\")\n",
    "                    break\n",
    "\n",
    "                # Process frame\n",
    "                processed_frame = self.process_frame(frame)\n",
    "                \n",
    "                # Display the frame\n",
    "                cv2.imshow('Hand-Wrist Detection', processed_frame)\n",
    "                \n",
    "                # Break loop on 'q' press\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in webcam detection: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if self.cap is not None:\n",
    "                self.cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            self.logger.info(\"Webcam detection stopped\")\n",
    "\n",
    "def main():\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # Initialize and run detector\n",
    "    detector = WebcamDetector()\n",
    "    detector.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
